{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from xml.etree import ElementTree as ET\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import XLNetTokenizer, XLNetModel, XLNetForSequenceClassification, RobertaTokenizer, AutoTokenizer, AutoModel, BertTokenizer, BertModel\nimport torch\nfrom torch import nn, optim\nimport pandas as pd\nimport numpy as np\nimport re\nimport os\nimport zipfile\nimport string\nfrom tqdm import tqdm\nfrom sklearn.metrics import f1_score,accuracy_score,confusion_matrix,mean_absolute_error\nfrom IPython.display import FileLink,FileLinks\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler,OneHotEncoder,LabelEncoder\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk import StanfordTagger\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-20T11:21:44.283285Z","iopub.execute_input":"2022-04-20T11:21:44.283597Z","iopub.status.idle":"2022-04-20T11:21:52.876285Z","shell.execute_reply.started":"2022-04-20T11:21:44.283525Z","shell.execute_reply":"2022-04-20T11:21:52.875501Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# **Task 1 : Hedge Cue Detection**","metadata":{}},{"cell_type":"markdown","source":"**Step-1 : HedgePeer Data Extraction and Dataframe Prep**","metadata":{}},{"cell_type":"code","source":"root = '../input'\nos.chdir(root)\ndata_path = 'hedgepeer/HedgePeer.jsonl'\n\ndataObj = pd.read_json(path_or_buf=data_path, lines=True)\ndata_list = []\nfor index, row in dataObj.iterrows():\n    rev_id = row['Review_id']\n    sents = row['Sentences']\n    for s in sents:\n        hedges = s['Hedges']\n        if(len(hedges)==0):\n            d = {}\n            d['Review_id'] = rev_id\n            d['Sentence_id'] = s['Sentence_id']\n            d['Raw Sentence'] = s['Sentence']\n            d['Hedged Sentence'] = s['Sentence']\n            d['Hedge'] = 'NO HEDGE'\n            d['Span'] = None\n            data_list.append(d)\n        else:\n            for h in hedges:\n                d = {}\n                d['Review_id'] = rev_id\n                d['Sentence_id'] = s['Sentence_id']\n                d['Raw Sentence'] = s['Sentence']\n                d['Hedged Sentence'] = h['Hedged Sentence']\n                d['Hedge'] = h['Hedge']\n                d['Span'] = h['Span']\n                data_list.append(d)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T11:23:05.670898Z","iopub.execute_input":"2022-04-20T11:23:05.671255Z","iopub.status.idle":"2022-04-20T11:23:06.675053Z","shell.execute_reply.started":"2022-04-20T11:23:05.671226Z","shell.execute_reply":"2022-04-20T11:23:06.674250Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(data_list)\ndf","metadata":{"execution":{"iopub.status.busy":"2022-04-20T11:23:08.993205Z","iopub.execute_input":"2022-04-20T11:23:08.993513Z","iopub.status.idle":"2022-04-20T11:23:09.113024Z","shell.execute_reply.started":"2022-04-20T11:23:08.993486Z","shell.execute_reply":"2022-04-20T11:23:09.112134Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"## Run -- To generate sentiment values for the data\n\nos.chdir('sentiment-intensity-prediction/generating-reviews-discovering-sentiment-master')\n\n## Load the OpenAI sentiment Model\nfrom encoder import Model\nmodel = Model()\n\nsent = list(df['Raw Sentence'])\ntext_features = model.transform(sent)\n\nsentiment_unit = text_features[:, 2388]\n\ndf['Sentiment Intensity'] = list(sentiment_unit)\nsentiment = df['Sentiment Intensity'].to_list()\n\nos.chdir(root)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T11:01:28.042500Z","iopub.execute_input":"2022-04-20T11:01:28.042989Z","iopub.status.idle":"2022-04-20T11:06:04.589057Z","shell.execute_reply.started":"2022-04-20T11:01:28.042947Z","shell.execute_reply":"2022-04-20T11:06:04.588280Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"## Run -- Standardizing sentiment values......\n\nx = df['Sentiment Intensity'].values.reshape(-1, 1) #returns a numpy array\nstd_scaler = StandardScaler()\nx_scaled = std_scaler.fit_transform(x)\ndf['Sentiment Intensity'] = list(x_scaled.reshape(-1))\ndf","metadata":{"execution":{"iopub.status.busy":"2022-04-20T11:06:04.590986Z","iopub.execute_input":"2022-04-20T11:06:04.591333Z","iopub.status.idle":"2022-04-20T11:06:04.636815Z","shell.execute_reply.started":"2022-04-20T11:06:04.591298Z","shell.execute_reply":"2022-04-20T11:06:04.635891Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"## Run -- Creating unique_id column......\n\nrev_id = df['Review_id']\nsen_id = df['Sentence_id']\nunq_id = [i+'_'+str(j) for i,j in zip(rev_id,sen_id)]\ndf['Unique_id'] = unq_id\ndf","metadata":{"execution":{"iopub.status.busy":"2022-04-20T11:23:20.982882Z","iopub.execute_input":"2022-04-20T11:23:20.983250Z","iopub.status.idle":"2022-04-20T11:23:21.044384Z","shell.execute_reply.started":"2022-04-20T11:23:20.983219Z","shell.execute_reply":"2022-04-20T11:23:21.043544Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df.to_csv('/kaggle/working/HedgePeer_sentiment.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-19T10:09:15.319916Z","iopub.execute_input":"2022-04-19T10:09:15.320327Z","iopub.status.idle":"2022-04-19T10:09:16.442213Z","shell.execute_reply.started":"2022-04-19T10:09:15.320289Z","shell.execute_reply":"2022-04-19T10:09:16.441419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Run\n\nunq_list = []\nsent_list = []\nhedged_sent_list = []\nhed_list = []\nspan_list = []\nsentiment_list=[]\n\ngp = df.groupby(by=['Unique_id'])\nfor name,grp in tqdm(gp):\n    sent_df = gp.get_group(name)\n    raw_sent = list(set(sent_df['Raw Sentence']))\n    sentiments = list(set(sent_df['Sentiment Intensity']))\n    hed_sent = list(sent_df['Hedged Sentence'])\n        \n    sent_hedges = list(sent_df['Hedge'])\n    sent_spans = list(sent_df['Span'])\n    sent_hed_span = [(i,j,k) for i,j,k in zip(hed_sent,sent_hedges,sent_spans) if j not in ['NO HEDGE','IDENT_PRECED']]\n        \n    hedged_sents = [i[0] for i in sent_hed_span]\n    hedges = [i[1] for i in sent_hed_span]\n    spans = [i[2] for i in sent_hed_span]\n        \n    unq_list.append(name)\n    sent_list.append(raw_sent)\n    hedged_sent_list.append(hedged_sents)\n    hed_list.append(hedges)\n    span_list.append(spans)\n    sentiment_list.append(sentiments)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T11:23:43.130355Z","iopub.execute_input":"2022-04-20T11:23:43.130675Z","iopub.status.idle":"2022-04-20T11:24:12.054022Z","shell.execute_reply.started":"2022-04-20T11:23:43.130647Z","shell.execute_reply":"2022-04-20T11:24:12.053109Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"spans","metadata":{"execution":{"iopub.status.busy":"2022-04-20T11:19:23.054775Z","iopub.execute_input":"2022-04-20T11:19:23.055097Z","iopub.status.idle":"2022-04-20T11:19:23.060764Z","shell.execute_reply.started":"2022-04-20T11:19:23.055070Z","shell.execute_reply":"2022-04-20T11:19:23.059669Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"## Run\ndata_dict = {'sentence_id':unq_list, 'sentence':sent_list, 'hedged_sentence':hedged_sent_list, 'speculative_cues':hed_list, 'scope_string':span_list, 'sentiment':sentiment_list}\ndata = pd.DataFrame(data_dict)\ndata","metadata":{"execution":{"iopub.status.busy":"2022-04-20T11:06:39.729766Z","iopub.execute_input":"2022-04-20T11:06:39.730066Z","iopub.status.idle":"2022-04-20T11:06:39.939821Z","shell.execute_reply.started":"2022-04-20T11:06:39.730039Z","shell.execute_reply":"2022-04-20T11:06:39.938831Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"## Run -- Convert hedged sentences to hashed sentences.....\n\nhash_sents = []\nfor index, row in data.iterrows():\n    temp_list = []\n    cues = row['speculative_cues']\n    raw_sent = row['sentence']\n    hed_sent = row['hedged_sentence']\n    span = row['scope_string']\n    \n    if(len(cues)>0):\n        for hs in hed_sent:\n            hs = hs.replace(\".\",\"\")\n            hs = hs.replace(\",\",\"\")\n            hs = hs.replace(\"-\",\"\")\n            hs = hs.replace(\"?\",\"\")\n            hs = hs.replace(\"<span>\", \"\")\n            hs = hs.replace(\"</span>\", \"\")\n            if(hs.find('<h>')>-1):\n                hs = hs.replace(\"<h>\", \"#\")\n                hs = hs.replace(\"</h>\", \"#\")\n            if(hs.find('<mh>')>-1):\n                hs = hs.replace(\"<mh>\", \"#\")\n                hs = hs.replace(\"</mh>\", \"#\")\n            temp_list.append(hs)\n    else:\n        temp_list.append(raw_sent[0])\n    \n    hash_sents.append(temp_list)\n\ndata['hashed_sentence'] = hash_sents\ndata","metadata":{"execution":{"iopub.status.busy":"2022-04-20T11:06:39.941564Z","iopub.execute_input":"2022-04-20T11:06:39.942137Z","iopub.status.idle":"2022-04-20T11:06:44.652588Z","shell.execute_reply.started":"2022-04-20T11:06:39.942094Z","shell.execute_reply":"2022-04-20T11:06:44.651696Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"final_hashed_sents = []\nfor index, row in data.iterrows():\n    cues = row['speculative_cues']\n    hash_sents = row['hashed_sentence']\n    if(len(cues)<=1):\n        final_hashed_sents.append(hash_sents[0])\n    else:\n        raw_sent_from_hash = hash_sents[0].replace('#','').split()\n        hash_sents = [i.split() for i in hash_sents]\n        cue_idxs = []\n        for hs,c in zip(hash_sents,cues):\n            hash_idx = [i for i,x in enumerate(hs) if x=='#']\n            if(len(hash_idx)<2):\n                print('LESS THAN 2 HASHES FOUND IN MULTIPLE CUE INSTANCE....')\n                continue\n            cue_idx = (hash_idx[0], hash_idx[1]-2)\n                \n            st_idx = cue_idx[0]\n            end_idx = cue_idx[1]+1\n            if(c.split()!=raw_sent_from_hash[st_idx:end_idx]):\n                i=0\n                while(i<3):\n                    i+=1\n                    if(end_idx+i<len(raw_sent_from_hash) and c.split()==raw_sent_from_hash[st_idx+i:end_idx+i]):\n                        cue_idx = (st_idx+i, end_idx+i-1)\n                        break\n                    if(st_idx-i>=0 and c.split()==raw_sent_from_hash[st_idx-i:end_idx-i]):\n                        cue_idx = (st_idx-i, end_idx-i-1)\n                        break\n                \n            cue_idxs.append(cue_idx)\n            \n        for cue_idx in cue_idxs:\n            raw_sent_from_hash[cue_idx[0]] = '# '+ raw_sent_from_hash[cue_idx[0]]\n            raw_sent_from_hash[cue_idx[1]] = raw_sent_from_hash[cue_idx[1]] + ' #'\n        \n        final_hashed_sents.append(' '.join(raw_sent_from_hash))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-20T11:06:44.653875Z","iopub.execute_input":"2022-04-20T11:06:44.654456Z","iopub.status.idle":"2022-04-20T11:06:48.771459Z","shell.execute_reply.started":"2022-04-20T11:06:44.654418Z","shell.execute_reply":"2022-04-20T11:06:48.770663Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"## Run\n\ndata['hashed_sentence'] = final_hashed_sents\ndata['sentence'] = [i[0] for i in data['sentence'].to_list()]\ndata['sentiment'] = [i[0] for i in data['sentiment'].to_list()]\n\n## To remove instance with raw sentence == Nan.....\nnon_nan_list = [type(i)==str for i in data['sentence'].to_list()]\ndata = data.loc[non_nan_list]\n\nsent1 = data['hashed_sentence']\nsen_list = None\ncues = data['speculative_cues'].to_list()\nsentiment = data['sentiment'].to_list()\ndata","metadata":{"execution":{"iopub.status.busy":"2022-04-20T11:06:48.772777Z","iopub.execute_input":"2022-04-20T11:06:48.773111Z","iopub.status.idle":"2022-04-20T11:06:48.896164Z","shell.execute_reply.started":"2022-04-20T11:06:48.773078Z","shell.execute_reply":"2022-04-20T11:06:48.895348Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"**Step-2 : Creating dataloaders**","metadata":{}},{"cell_type":"code","source":"## Run -- One-Hot Rep for POS tags\n\ntag_set = np.array(['CC','CD','DT','EX','FW','IN','JJ','JJR','JJS','LS','MD','NN','NNS','NNP','NNPS','PDT','POS','PRP','PRP$','RB','RBR','RBS','RP','SYM','TO','UH','VB','VBD','VBG','VBN','VBP','VBZ','WDT','WP','WP$','WRB','$',\"''\"])\nnum_tags = tag_set.shape[0]\n\nlabel_encoder = LabelEncoder()\ninteger_encoded = label_encoder.fit_transform(tag_set)\nchar_to_int = dict((c, i) for c,i in zip(tag_set,integer_encoded))\nint_to_char = dict((i, c) for c,i in zip(tag_set,integer_encoded))\nprint(integer_encoded)\nonehot_encoder = OneHotEncoder(sparse=False)\ninteger_encoded = integer_encoded.reshape(len(integer_encoded), 1)\nonehot_encoded = onehot_encoder.fit_transform(integer_encoded)\none_hot_labels = dict((c,list(i)) for c,i in zip(tag_set,onehot_encoded))\nprint(onehot_encoded)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T11:06:48.914489Z","iopub.execute_input":"2022-04-20T11:06:48.915291Z","iopub.status.idle":"2022-04-20T11:06:48.928849Z","shell.execute_reply.started":"2022-04-20T11:06:48.915233Z","shell.execute_reply":"2022-04-20T11:06:48.928017Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"num_tags","metadata":{"execution":{"iopub.status.busy":"2022-04-20T11:06:48.931435Z","iopub.execute_input":"2022-04-20T11:06:48.932075Z","iopub.status.idle":"2022-04-20T11:06:48.938713Z","shell.execute_reply.started":"2022-04-20T11:06:48.932031Z","shell.execute_reply":"2022-04-20T11:06:48.937690Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"## Run\n## 0=not a cue, 1=normal cue, 2=multiword cue, 3=<pad> token \n\nclass Bio_dataset(Dataset):\n    def __init__(self,sentences,cues,trans_model,sentiment,tokenizer,max_len,pos_tagging,num_tags,pos_scaff):\n        self.sent = sentences\n        self.trans_model = trans_model\n        self.token = tokenizer\n        self.max = max_len\n        self.cues = cues\n        self.sentiment = sentiment\n        self.pos_tagging = pos_tagging\n        self.num_tags = num_tags\n        self.pos_scaff = pos_scaff\n    def __len__(self):\n        return len(self.sent)\n    def tokenids_gen(self):\n        targets = []\n        senids=[]\n        attention_masks=[]\n        pos_tags=[]\n        pad_token_ids = {'xlnet':5,'bert':0,'scibert':0}\n        for s,c in zip(self.sent,self.cues):\n            encodings = tokenizer.encode_plus(s,\n                                  return_tensors='pt',\n                                  truncation=False,\n                                  return_token_type_ids=True,\n                                  return_attention_mask=True,\n                                  )\n            \n            att = list(encodings['attention_mask'][0])\n            senid = list(encodings['input_ids'][0])\n            att = [i.item() for i in att]\n            senid = [i.item() for i in senid]\n            k = [tokenizer.decode(i) for i in senid]\n\n            tar = [0 for i in range(len(k))]\n            idxend=-1\n            for cue in c:\n                idxstart = k.index('#',idxend+1)\n                idxend = k.index('#',idxstart+1)\n                tar[idxstart] = -1\n                tar[idxend] = -1\n                if(len(cue.split())>1):\n                    tar[idxstart+1:idxend] = [2 for i in range(idxend-idxstart-1)]\n                else:\n                    tar[idxstart+1:idxend] = [1 for i in range(idxend-idxstart-1)]\n                    \n            ## loop to delete '' or '#' element from the sent list (here sent list = list of sentences with each one having multiple cues marked with #) \n            for i in range(2*len(c)):\n                idx = k.index('#')\n                if(k[idx-1]==''):\n                    del k[idx-1:idx+1]\n                    del senid[idx-1:idx+1]\n                    del tar[idx-1:idx+1]\n                    del att[idx-1:idx+1]\n                else:\n                    del k[idx]\n                    del senid[idx]\n                    del tar[idx]\n                    del att[idx]\n                \n            senid = [i for i,j in zip(senid,k) if re.search('[A-Za-z0-9]+', j)!=None]\n            tar = [i for i,j in zip(tar,k) if re.search('[A-Za-z0-9]+', j)!=None]\n            att = [i for i,j in zip(att,k) if re.search('[A-Za-z0-9]+', j)!=None]\n            k = [i for i in k if re.search('[A-Za-z0-9]+', i)!=None]\n\n            ## adding pad token at the end....\n            tar = tar+[3 for i in range(self.max - len(k))]\n            senid = senid+[pad_token_ids[self.trans_model] for i in range(self.max - len(k))]\n            att = att+[0 for i in range(self.max - len(k))]\n    \n            if(self.pos_tagging == True and self.pos_scaff == False):\n                tagged = nltk.pos_tag(k)\n                pos_tag_labels = [one_hot_labels[i[1]] for i in tagged]\n                pad_encod = [0 for i in range(self.num_tags)]\n                pos_tag_labels = pos_tag_labels+[pad_encod for i in range(self.max - len(k))]\n                pos_tags.append(pos_tag_labels)\n            \n            if(self.pos_tagging == True and self.pos_scaff == True):\n                tagged = nltk.pos_tag(k)\n                pos_tag_labels = [char_to_int[i[1]] for i in tagged]\n                pad_encod = self.num_tags\n                pos_tag_labels = pos_tag_labels+[pad_encod for i in range(self.max - len(k))]\n                pos_tags.append(pos_tag_labels)\n                \n            targets.append(tar)\n            senids.append(senid)\n            attention_masks.append(att)\n            \n        return (senids,attention_masks,targets,self.sentiment,pos_tags)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T11:06:48.940387Z","iopub.execute_input":"2022-04-20T11:06:48.941047Z","iopub.status.idle":"2022-04-20T11:06:48.969150Z","shell.execute_reply.started":"2022-04-20T11:06:48.941009Z","shell.execute_reply":"2022-04-20T11:06:48.968312Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"## Run -- Creates dictionary with input_tokens, att_mask, targets tensors\nclass Dataset_gen(Dataset):\n\n    def __init__(self,sentences,targets,att_masks,sentiment,pos_tags):\n        self.sent = sentences\n        self.tar = targets\n        self.att = att_masks\n        self.sentiment = sentiment\n        self.pos_tags = pos_tags\n    \n    def __len__(self):\n        return len(self.sent)\n    \n    def __getitem__(self, item):\n        sent = torch.tensor(self.sent[item])\n        target = torch.tensor(self.tar[item])\n        att = torch.tensor(self.att[item])\n        sentiment = torch.tensor(self.sentiment[item])\n        if(len(self.pos_tags)!=0):\n            pos_tags = torch.tensor(self.pos_tags[item])\n            ret_dict = {'input':sent,'attention_mask':att,'targets':target,'sentiment':sentiment,'pos_tags':pos_tags}\n        else:\n            ret_dict = {'input':sent,'attention_mask':att,'targets':target,'sentiment':sentiment}\n        return ret_dict","metadata":{"execution":{"iopub.status.busy":"2022-04-20T11:06:48.972933Z","iopub.execute_input":"2022-04-20T11:06:48.973188Z","iopub.status.idle":"2022-04-20T11:06:48.984811Z","shell.execute_reply.started":"2022-04-20T11:06:48.973164Z","shell.execute_reply":"2022-04-20T11:06:48.983900Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"## Run\n\n# for task 1 => data2mark = (cues,sentiment)\n# for task 2 => data2mark = (scope,sentiment)\n\ndef dataloader_gen(sent,data2mark,trans_model,tokenizer,max_len,batch_size,task,pos_tagging,num_tags,pos_scaff):\n    if(task==1):\n        cues = [i[0] for i in data2mark]\n        sentiment = [i[1] for i in data2mark]\n        if(pos_tagging==True):\n            b = Bio_dataset(sent,cues,trans_model,sentiment,tokenizer,max_len,pos_tagging=pos_tagging,num_tags=num_tags,pos_scaff=pos_scaff)\n        else:\n            b = Bio_dataset(sent,cues,trans_model,sentiment,tokenizer,max_len,pos_tagging=False,num_tags=None,pos_scaff=False)\n    else:\n        spans = [i[0] for i in data2mark]\n        sentiment = [i[1] for i in data2mark]\n        if(pos_tagging==True):\n            b = Biot2_dataset(sent,spans,trans_model,sentiment,tokenizer,max_len,pos_tagging=pos_tagging,num_tags=num_tags,pos_scaff=pos_scaff)\n        else:\n            b = Biot2_dataset(sent,spans,trans_model,sentiment,tokenizer,max_len,pos_tagging=False,num_tags=None,pos_scaff=False)\n    \n    x,att,y,ys,pos_tags = b.tokenids_gen()\n    data = Dataset_gen(x,y,att,ys,pos_tags)\n    \n    return DataLoader(data,batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T11:06:48.987795Z","iopub.execute_input":"2022-04-20T11:06:48.988112Z","iopub.status.idle":"2022-04-20T11:06:48.999227Z","shell.execute_reply.started":"2022-04-20T11:06:48.988087Z","shell.execute_reply":"2022-04-20T11:06:48.998326Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"## leng_more = list of indices with sent tokens length > max_len\n\ndef remove_big_instances(data,sen_list,sent,data2mark,sentiment,tokenizer,max_len):\n    l = list(data['sentence'])\n    leng_more = [i[0] for i in enumerate(l) if len(tokenizer.encode_plus(i[1],truncation=False,return_token_type_ids=True,return_attention_mask=True)['input_ids'])>max_len]\n    if(sen_list!=None):\n        sen_list = [i[1] for i in enumerate(sen_list) if i[0] not in leng_more]\n    sent = [i[1] for i in enumerate(sent) if i[0] not in leng_more]\n    data2mark = [i[1] for i in enumerate(data2mark) if i[0] not in leng_more]\n    sentiment = [i[1] for i in enumerate(sentiment) if i[0] not in leng_more]\n    return (leng_more,sen_list,sent,data2mark,sentiment)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T11:06:49.000813Z","iopub.execute_input":"2022-04-20T11:06:49.001275Z","iopub.status.idle":"2022-04-20T11:06:49.017064Z","shell.execute_reply.started":"2022-04-20T11:06:49.001221Z","shell.execute_reply":"2022-04-20T11:06:49.016091Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"len(sent1)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T11:06:49.019106Z","iopub.execute_input":"2022-04-20T11:06:49.019417Z","iopub.status.idle":"2022-04-20T11:06:49.029522Z","shell.execute_reply.started":"2022-04-20T11:06:49.019392Z","shell.execute_reply":"2022-04-20T11:06:49.028759Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"## Run \n# remove instances larger than max_len\n\ntrans_model = 'xlnet'\n\ntokenizer1 = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n\ntokenizer2 = BertTokenizer.from_pretrained('bert-base-cased')\n\ntokenizer3 = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_cased')\n\ntokenizer_dict = {'xlnet':tokenizer1,'bert':tokenizer2,'scibert':tokenizer3}\n\ntokenizer = tokenizer_dict[trans_model]\n\nlen_more,sen_list,sent1,cues,sentiment = remove_big_instances(data,sen_list,sent1,cues,sentiment,tokenizer,100)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T11:06:49.033303Z","iopub.execute_input":"2022-04-20T11:06:49.033753Z","iopub.status.idle":"2022-04-20T11:07:09.501601Z","shell.execute_reply.started":"2022-04-20T11:06:49.033725Z","shell.execute_reply":"2022-04-20T11:07:09.500781Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"len(len_more)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T11:07:09.505051Z","iopub.execute_input":"2022-04-20T11:07:09.505339Z","iopub.status.idle":"2022-04-20T11:07:09.510440Z","shell.execute_reply.started":"2022-04-20T11:07:09.505313Z","shell.execute_reply":"2022-04-20T11:07:09.509633Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"## Run -- Length after removing max_len instances\nlen(sent1)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T11:07:09.511669Z","iopub.execute_input":"2022-04-20T11:07:09.512311Z","iopub.status.idle":"2022-04-20T11:07:09.521453Z","shell.execute_reply.started":"2022-04-20T11:07:09.512273Z","shell.execute_reply":"2022-04-20T11:07:09.520380Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"## Run : Loading data into DataLoaders\n\n'''\nValue of Tags for different architectures --\n1. Simple Baseline = \npos_tagging = False, pos_scaff = False, sent_scaff = False\n2. MTL with sentiment scaff = \npos_tagging = False, pos_scaff = False, sent_scaff = True\n1. MTL with sentiment + POS scaffs = \npos_tagging = True, pos_scaff = True, sent_scaff = True\n1. MTL with sentiment scaff + POS Late Fusion (LF) = \npos_tagging = True, pos_scaff = False, sent_scaff = True\n'''\n\ndata = 'hedgepeer'\npos_tagging=False\npos_scaff=False\nsent_scaff=False\nnum_tags=num_tags\n\nmax_len = 100\nbatch_size = 4\n\nif(data=='hedgepeer'):\n    # split (train,val,test) = (70%,20%,10%)\n    train_size = 36924\n    val_size = 10552\n    test_size = len(sent1)-train_size-val_size\n\nelse:\n    print('Wrong Dataset Entry!')\n\ny12 = [(c,s) for c,s in zip(cues,sentiment)]\nsen_train, sen_test, y12_train, y12_test = train_test_split(sent1,y12,test_size=test_size, random_state=0)\nsen_train, sen_val, y12_train, y12_val = train_test_split(sen_train,y12_train,test_size=val_size, random_state=0)\n    \ntrain_data_loader = dataloader_gen(sen_train,y12_train,trans_model,tokenizer,max_len,batch_size,task=1,pos_tagging=pos_tagging,num_tags=num_tags,pos_scaff=pos_scaff)\nval_data_loader = dataloader_gen(sen_val,y12_val,trans_model,tokenizer,max_len,batch_size,task=1,pos_tagging=pos_tagging,num_tags=num_tags,pos_scaff=pos_scaff)\ntest_data_loader = dataloader_gen(sen_test,y12_test,trans_model,tokenizer,max_len,batch_size,task=1,pos_tagging=pos_tagging,num_tags=num_tags,pos_scaff=pos_scaff)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-20T11:11:21.829114Z","iopub.execute_input":"2022-04-20T11:11:21.829479Z","iopub.status.idle":"2022-04-20T11:12:13.928732Z","shell.execute_reply.started":"2022-04-20T11:11:21.829450Z","shell.execute_reply":"2022-04-20T11:12:13.927821Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"len(sen_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T11:12:13.930389Z","iopub.execute_input":"2022-04-20T11:12:13.930715Z","iopub.status.idle":"2022-04-20T11:12:13.937605Z","shell.execute_reply.started":"2022-04-20T11:12:13.930681Z","shell.execute_reply":"2022-04-20T11:12:13.936775Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"len(sen_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T10:34:12.069732Z","iopub.execute_input":"2022-04-19T10:34:12.070177Z","iopub.status.idle":"2022-04-19T10:34:12.07611Z","shell.execute_reply.started":"2022-04-19T10:34:12.070135Z","shell.execute_reply":"2022-04-19T10:34:12.075054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Task 1 : Hedge cues detection Model**","metadata":{}},{"cell_type":"markdown","source":"**Simple Baseline**","metadata":{}},{"cell_type":"code","source":"## XLnet model\nclass cue_model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.xlmodel = XLNetModel.from_pretrained('xlnet-base-cased')\n        self.lin = nn.Linear(768,4)\n        \n    def forward(self,x,att):\n        xl=self.xlmodel(x,attention_mask=att)[0]\n        xl = xl.view(-1,xl.shape[2])\n        lin = self.lin(xl)\n        return (lin)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T11:12:13.939016Z","iopub.execute_input":"2022-04-20T11:12:13.939711Z","iopub.status.idle":"2022-04-20T11:12:13.947456Z","shell.execute_reply.started":"2022-04-20T11:12:13.939672Z","shell.execute_reply":"2022-04-20T11:12:13.946759Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"## Scibert model\nclass cue_model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.xlmodel = AutoModel.from_pretrained('allenai/scibert_scivocab_cased')\n        self.lin = nn.Linear(768,4)\n        \n    def forward(self,x,att):\n        xl=self.xlmodel(x,attention_mask=att)[0]\n        xl = xl.view(-1,xl.shape[2])\n        lin = self.lin(xl)\n        return (lin)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Bert model\nclass cue_model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.xlmodel = BertModel.from_pretrained('bert-base-cased')\n        self.lin = nn.Linear(768,4)\n        \n    def forward(self,x,att):\n        xl=self.xlmodel(x,attention_mask=att)[0]\n        xl = xl.view(-1,xl.shape[2])\n        lin = self.lin(xl)\n        return (lin)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Multi Task Learning Model**","metadata":{}},{"cell_type":"code","source":"## Attention class\nclass attention(nn.Module):\n    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n        super(attention, self).__init__(**kwargs)\n        \n        self.supports_masking = True\n\n        self.bias = bias\n        self.feature_dim = feature_dim\n        self.step_dim = step_dim\n        self.features_dim = 0\n        self.a = 0\n        self.th = 0\n        self.eij = 0\n        \n        weight = torch.zeros(feature_dim, 1)\n        nn.init.kaiming_uniform_(weight)\n        self.weight = nn.Parameter(weight)\n        \n        if bias:\n            self.b = nn.Parameter(torch.zeros(step_dim))\n        \n    def forward(self, x, mask=None):\n        feature_dim = self.feature_dim \n        step_dim = self.step_dim\n\n        self.eij = torch.mm(\n            x.contiguous().view(-1, feature_dim), \n            self.weight\n        ).view(-1, step_dim)\n        \n        if self.bias:\n            self.eij = self.eij + self.b\n            \n        self.th = torch.tanh(self.eij)\n        a = torch.exp(self.th)\n        \n        if mask is not None:\n            a = a * mask\n\n        self.a = a / (torch.sum(a, 1, keepdim=True) + 1e-10)\n\n        weighted_input = x * torch.unsqueeze(self.a, -1)\n        return torch.sum(weighted_input, 1)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T10:53:03.531329Z","iopub.execute_input":"2022-04-19T10:53:03.531655Z","iopub.status.idle":"2022-04-19T10:53:03.541692Z","shell.execute_reply.started":"2022-04-19T10:53:03.531627Z","shell.execute_reply":"2022-04-19T10:53:03.540839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## MAIN TASK WITHOUT POS_TAGGING........\n# feed forward of main task (hedge cue/span prediction)\n\nclass main_head_1(nn.Module):\n    def __init__(self,lstm_hidden_size):\n        super().__init__()\n        self.lin = nn.Linear(2*lstm_hidden_size,16)\n        self.drop = nn.Dropout(p=0.4)\n        self.out = nn.Linear(16,4)\n        self.relu = torch.nn.ReLU()\n    def forward(self,lstm):\n        lstm = torch.reshape(lstm, (-1, lstm.shape[2]))\n        drop = self.drop(lstm)\n        lin = self.lin(drop)\n        lin = self.relu(lin)\n        out = self.out(lin)\n        return (out) ","metadata":{"execution":{"iopub.status.busy":"2022-04-19T10:57:56.066378Z","iopub.execute_input":"2022-04-19T10:57:56.066719Z","iopub.status.idle":"2022-04-19T10:57:56.073681Z","shell.execute_reply.started":"2022-04-19T10:57:56.066691Z","shell.execute_reply":"2022-04-19T10:57:56.072553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## MAIN TASK WITH POS_TAGGING........\n# feed forward of main task (hedge cue/span prediction)\n\nclass main_head_2(nn.Module):\n    def __init__(self,lstm_hidden_size,pos_tagging,num_tags):\n        super().__init__()\n        self.lin = nn.Linear(2*lstm_hidden_size,16)\n        self.out = nn.Linear(16+num_tags,4)\n        self.pos_tagging = pos_tagging\n    def forward(self,pos_tags,lstm):\n        lstm = torch.reshape(lstm, (-1, lstm.shape[2]))\n        pos_tags = pos_tags.view(-1,pos_tags.shape[2])\n        lin = self.lin(lstm)\n        pos_lin = torch.cat((lin,pos_tags),1).float()\n        out = self.out(pos_lin)\n        return (out) ","metadata":{"execution":{"iopub.status.busy":"2022-04-19T10:57:57.800398Z","iopub.execute_input":"2022-04-19T10:57:57.800715Z","iopub.status.idle":"2022-04-19T10:57:57.807928Z","shell.execute_reply.started":"2022-04-19T10:57:57.800686Z","shell.execute_reply":"2022-04-19T10:57:57.807059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feed forward of sentiment task \nclass sentiment_head(nn.Module):\n    def __init__(self,hidden_size,max_len):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.att = attention(2*hidden_size,max_len)   ## here second argument is max_len (here max_len = 100)\n        self.out = nn.Linear(2*hidden_size,1)\n    def forward(self,lstm):\n        at = self.att(lstm).view(-1,2*self.hidden_size)\n        x = self.out(at)\n        return (x) ","metadata":{"execution":{"iopub.status.busy":"2022-04-19T10:53:27.926314Z","iopub.execute_input":"2022-04-19T10:53:27.926634Z","iopub.status.idle":"2022-04-19T10:53:27.93275Z","shell.execute_reply.started":"2022-04-19T10:53:27.926602Z","shell.execute_reply":"2022-04-19T10:53:27.931879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## POS TASK........\n# feed forward of pos task\nclass pos_head(nn.Module):\n    def __init__(self,lstm_hidden_size,num_tags):\n        super().__init__()\n        self.lin = nn.Linear(2*lstm_hidden_size,num_tags)\n    def forward(self,lstm):\n        lstm = torch.reshape(lstm, (-1, lstm.shape[2]))\n        lin = self.lin(lstm)\n        return (lin) ","metadata":{"execution":{"iopub.status.busy":"2022-04-19T10:53:37.521856Z","iopub.execute_input":"2022-04-19T10:53:37.52234Z","iopub.status.idle":"2022-04-19T10:53:37.527782Z","shell.execute_reply.started":"2022-04-19T10:53:37.5223Z","shell.execute_reply":"2022-04-19T10:53:37.526869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## MTL model\nclass cue_MTL_model(nn.Module):\n    def __init__(self,trans_model,hidden_size,max_len,num_tags,pos_tagging=False,pos_scaff=False):\n        super().__init__()\n        if(trans_model=='xlnet'):\n            self.trans = XLNetModel.from_pretrained('xlnet-base-cased')\n        if(trans_model=='scibert'):\n            self.trans = AutoModel.from_pretrained('allenai/scibert_scivocab_cased')\n        if(trans_model=='bert'):\n            self.trans = BertModel.from_pretrained('bert-base-cased')\n        \n        self.lstm = nn.LSTM(768,hidden_size,num_layers=1,bidirectional=True,batch_first=True)\n        \n        if(pos_tagging==True and pos_scaff==False):\n            self.main = main_head_2(lstm_hidden_size=hidden_size,pos_tagging=pos_tagging,num_tags=num_tags)\n        else:\n            self.main = main_head_1(lstm_hidden_size=hidden_size)\n        \n        if(pos_tagging==True and pos_scaff==True):\n            self.pos = pos_head(lstm_hidden_size=hidden_size,num_tags=num_tags)\n            \n        self.sentiment = sentiment_head(hidden_size,max_len)\n        self.pos_tagging = pos_tagging\n        \n    def forward(self,x,att,pos_tags=None):\n        xl=self.trans(x,attention_mask=att)[0]\n        lstm,_=self.lstm(xl)\n        if(self.pos_tagging==True and pos_scaff==False):\n            main_out = self.main(pos_tags,lstm)\n        else:\n            main_out = self.main(lstm)\n        \n        sentiment_out = self.sentiment(lstm)\n        \n        if(pos_tagging==True and pos_scaff==True):\n            pos_out = self.pos(lstm)\n            return (main_out,sentiment_out,pos_out)\n            \n        else:\n            return (main_out,sentiment_out)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T10:59:54.797316Z","iopub.execute_input":"2022-04-19T10:59:54.797624Z","iopub.status.idle":"2022-04-19T10:59:54.809083Z","shell.execute_reply.started":"2022-04-19T10:59:54.797596Z","shell.execute_reply":"2022-04-19T10:59:54.80806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**MTL Sentiment scaff + POS Late Fusion Model**","metadata":{}},{"cell_type":"code","source":"## MTL model\nclass cue_LF_model(nn.Module):\n    def __init__(self,trans_model,max_len,num_tags,pos_tagging=False,sentiment_lf=False):\n        super().__init__()\n        if(trans_model=='xlnet'):\n            self.trans = XLNetModel.from_pretrained('xlnet-base-cased')\n        if(trans_model=='scibert'):\n            self.trans = AutoModel.from_pretrained('allenai/scibert_scivocab_cased')\n        if(trans_model=='bert'):\n            self.trans = BertModel.from_pretrained('bert-base-cased')\n                \n        self.lin = nn.Linear(768,16)\n        self.out = nn.Linear(16+num_tags+1,4)            \n        self.pos_tagging = pos_tagging\n        self.sentiment_lf = sentiment_lf\n        \n    def forward(self,x,att,pos_tags=None,sentiment=None):\n        xl=self.trans(x,attention_mask=att)[0]\n        xl = xl.view(-1,xl.shape[2])\n        lin = self.lin(xl)\n        pos_lin = torch.cat((lin,pos_tags),1).float()\n\n        xl=self.trans(x,attention_mask=att)[0]\n#         print('\\ntrans model output shape : ',xl.shape)\n        lstm,_=self.lstm(xl)\n#         print('lstm shape : ',lstm.shape)\n        if(self.pos_tagging==True and pos_scaff==False):\n            main_out = self.main(pos_tags,lstm)\n#             print('main_out shape : ',main_out.shape)\n        else:\n            main_out = self.main(lstm)\n        \n        sentiment_out = self.sentiment(lstm)\n        \n        if(pos_tagging==True and pos_scaff==True):\n            pos_out = self.pos(lstm)\n#             print('pos_out shape : ',pos_out.shape)\n            return (main_out,sentiment_out,pos_out)\n            \n        else:\n            return (main_out,sentiment_out)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Training**","metadata":{}},{"cell_type":"markdown","source":"**Simple Baseline**","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T11:14:02.801531Z","iopub.execute_input":"2022-04-20T11:14:02.801888Z","iopub.status.idle":"2022-04-20T11:14:02.809136Z","shell.execute_reply.started":"2022-04-20T11:14:02.801856Z","shell.execute_reply":"2022-04-20T11:14:02.807956Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"model = cue_model()\nmodel.to(device)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-20T11:14:04.370629Z","iopub.execute_input":"2022-04-20T11:14:04.370949Z","iopub.status.idle":"2022-04-20T11:14:28.562848Z","shell.execute_reply.started":"2022-04-20T11:14:04.370920Z","shell.execute_reply":"2022-04-20T11:14:28.562151Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"for i,d in enumerate(val_data_loader):\n    inp = d['input'].to(device)\n    att = d['attention_mask'].to(device)\n    targets = d['targets'].to(device)\n    sentiment = d['sentiment'].to(device)\n    print(targets.shape)\n    targets = targets.view(-1)\n    print(targets.shape)\n    print(sentiment.shape)\n    break","metadata":{"execution":{"iopub.status.busy":"2022-04-19T10:38:50.388154Z","iopub.execute_input":"2022-04-19T10:38:50.388501Z","iopub.status.idle":"2022-04-19T10:38:50.406989Z","shell.execute_reply.started":"2022-04-19T10:38:50.388469Z","shell.execute_reply":"2022-04-19T10:38:50.405788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Run\n\ndef evaluate(model,val_data,task,pos_tagging,pos_scaff,sent_scaff):\n    model.eval()\n    model.to(device)\n    main_loss = 0\n    true=[]\n    pred=[]\n    with torch.no_grad():\n        for i,d in enumerate(val_data):\n            inp = d['input'].to(device)\n            att = d['attention_mask'].to(device)\n            targets = d['targets'].view(-1).to(device)\n            \n            if(pos_tagging==True and pos_scaff==False):\n                pos_tags = d['pos_tags'].to(device)\n                logits = model(inp,att,pos_tags)[0]\n            elif(sent_scaff==False and pos_tagging==False and pos_scaff==False):\n                logits = model(inp,att)   ## For simple baseline\n            else:\n                logits = model(inp,att)[0]   ## For sentiment MTL\n\n            loss = cse_loss(logits,targets)\n            main_loss += loss.item()\n            \n            _,predictions = torch.max(logits,dim=1)\n            \n            targets = targets.cpu().detach().numpy()\n            predictions = predictions.cpu().detach().numpy()\n            \n            if(task==1):\n                tr,pr = remove_pad_pred_t1(targets,predictions)\n                true += tr\n                pred += pr\n            else:\n                true += list(targets)\n                pred += list(predictions)\n\n        main_loss = main_loss/(i+1)\n    return (main_loss,true,pred)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T11:14:28.564655Z","iopub.execute_input":"2022-04-20T11:14:28.564992Z","iopub.status.idle":"2022-04-20T11:14:28.575076Z","shell.execute_reply.started":"2022-04-20T11:14:28.564959Z","shell.execute_reply":"2022-04-20T11:14:28.574000Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def remove_pad_pred_t1(true,pred):\n    idx = [i for i,d in enumerate(true) if d==3]\n    true = [i for i in true if i!=3]\n    pred = [d for i,d in enumerate(pred) if i not in idx]\n    return (true,pred)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T11:14:28.576771Z","iopub.execute_input":"2022-04-20T11:14:28.577217Z","iopub.status.idle":"2022-04-20T11:14:28.584749Z","shell.execute_reply.started":"2022-04-20T11:14:28.577177Z","shell.execute_reply":"2022-04-20T11:14:28.583846Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"## Run --- Training Cue Detcetion Model -- SIMPLE BASELINE........\nepochs = 1\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\nweights = torch.tensor([1., 1., 1., 0.]).to(device)\ncse_loss = torch.nn.CrossEntropyLoss(weight=weights)\n\nloss_list=[]\nfor ep in range(epochs):\n    total_loss=0\n    true=[]\n    pred=[]\n    model.train()\n    \n    for i,d in enumerate(train_data_loader):\n        if(i%300 == 299):\n            print('batch - ',i+1)\n\n        inp = d['input'].to(device)\n        att = d['attention_mask'].to(device)\n        targets = d['targets'].view(-1).to(device)\n\n        logits = model(inp,att)\n\n        loss = cse_loss(logits,targets)\n            \n        _,predictions = torch.max(logits,dim=1)\n\n        targets = targets.cpu().detach().numpy()\n        predictions = predictions.cpu().detach().numpy()\n        \n        tr,pr = remove_pad_pred_t1(targets,predictions)\n        \n        true += tr\n        pred += pr\n\n        total_loss += loss.item()\n            \n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        \n            \n    total_loss = total_loss/(i+1)\n\n    f1 = f1_score(true,pred,average='macro')\n    acc = accuracy_score(true, pred)\n    print('epoch : ',ep+1,' --','\\n','loss : ',total_loss,'\\t','f1 : ',f1,'\\t','acc : ',acc)\n    print('train confusion matrix :')\n    print(confusion_matrix(true,pred))\n    \n    # validation \n    val_loss,val_true,val_pred = evaluate(model=model,val_data=val_data_loader,task=1,pos_tagging=False,pos_scaff=False,sent_scaff=False)\n        \n    val_f1 = f1_score(val_true,val_pred,average='macro')\n    val_acc = accuracy_score(val_true, val_pred)\n    print('val loss : ',val_loss,'\\t','val_f1 : ',val_f1,'\\t','val_acc : ',val_acc)\n    print('val confusion matrix :')\n    print(confusion_matrix(val_true,val_pred))\n    \n    torch.save(model, f'/kaggle/working/xlnet_{data}_cue_only_model_ep{ep+1}.pt')\n    \n    loss_list.append({'train_loss':total_loss,'val_loss':val_loss})","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-20T11:14:51.430824Z","iopub.execute_input":"2022-04-20T11:14:51.431166Z","iopub.status.idle":"2022-04-20T11:14:51.519324Z","shell.execute_reply.started":"2022-04-20T11:14:51.431136Z","shell.execute_reply":"2022-04-20T11:14:51.516109Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"torch.save(model, f'/kaggle/working/xlnet_{data}_cue_only_model_ep{ep+1}.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Run -- Results on HedgePeer Test Data\nprint('BERT MODEL RESULTS ON HEDGEPEER TEST DATA')\nroot = '/kaggle/working'\nfor model_name in os.listdir(root):\n    model_path = root+model_name\n    if(model_name[:14]!='bert_hedgepeer'):\n        continue\n    model = torch.load(model_path)\n    model.to(device)\n    test_loss,test_true,test_pred = evaluate(model,test_data_loader,task=1)\n\n    test_f1 = f1_score(test_true,test_pred,average='macro')\n    test_acc = accuracy_score(test_true, test_pred)\n    print(f'model : {model_name}')\n    print('test loss : ',test_loss,'\\t','test_f1 : ',test_f1,'\\t','test_acc : ',test_acc)\n    print('test confusion matrix :')\n    print(confusion_matrix(test_true,test_pred))\n    print('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**MTL model**","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-04-20T11:15:00.067147Z","iopub.execute_input":"2022-04-20T11:15:00.067525Z","iopub.status.idle":"2022-04-20T11:15:00.072484Z","shell.execute_reply.started":"2022-04-20T11:15:00.067493Z","shell.execute_reply":"2022-04-20T11:15:00.071542Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"pos_tagging = False\nhidden_size=100\npos_scaff = False\nsent_scaff = True\n\nif(pos_tagging == False and pos_scaff == False):\n    num_tags = None\n\nif(pos_tagging == True and pos_scaff == True):\n    num_tags = num_tags+1           # To take into account the label for the padding token.......\n\nmodel = cue_MTL_model(trans_model=trans_model,hidden_size=hidden_size,max_len=max_len,num_tags=num_tags,pos_tagging=pos_tagging,pos_scaff=pos_scaff)\nmodel.to(device)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-19T11:00:01.70879Z","iopub.execute_input":"2022-04-19T11:00:01.709145Z","iopub.status.idle":"2022-04-19T11:00:06.041458Z","shell.execute_reply.started":"2022-04-19T11:00:01.709116Z","shell.execute_reply":"2022-04-19T11:00:06.039648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Run --- Training Cue Detcetion Model -- MTL MODEL........\nepochs = 7\n\n# lambd1 = Lambd1 for main task, lambd2 = Lambda for sentiment task, lambd3 = Lambda for POS task\nlambd1 = 1\nlambd2 = 0.1\nlambd3 = 0.05\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\nweights = torch.tensor([1., 1., 1., 0.]).to(device)\nif(pos_tagging==True and pos_scaff==True):\n    pos_weights = torch.tensor([float(1) for i in range(num_tags-1)]+[float(0)]).to(device)     # weight = 0 for padding token\n    pos_cse_loss = torch.nn.CrossEntropyLoss(weight=pos_weights)\ncse_loss = torch.nn.CrossEntropyLoss(weight=weights)\nmse_loss = torch.nn.MSELoss()\n\nloss_list=[]\nfor ep in range(epochs):\n    running_loss=0\n    ep_main_loss=0\n    ep_sentiment_loss=0\n    ep_pos_loss=0\n    true=[]\n    pred=[]\n    sent_true=[]\n    sent_pred=[]\n    pos_true=[]\n    pos_pred=[]\n    model.train()\n    \n    for i,d in enumerate(train_data_loader):\n        if(i%300 == 299):\n            print('batch - ',i+1)\n\n        inp = d['input'].to(device)\n        att = d['attention_mask'].to(device)\n        targets = d['targets'].view(-1).to(device)           # targets shape : (batch_size,max_len)\n        sentiment = d['sentiment'].to(device)\n        \n        if(pos_tagging==True):\n            pos_tags = d['pos_tags'].to(device)              # pos_tags shape for pos_scaff==False : (batch_size,max_len,num_tags)\n            if(pos_scaff==False):\n                main_logits,sentiment_logits = model(inp,att,pos_tags)\n            else:\n                pos_tags = pos_tags.view(-1)                 # pos_tags shape before view : (batch_size,max_len)\n                main_logits,sentiment_logits,pos_logits = model(inp,att)\n\n        else:\n            main_logits,sentiment_logits = model(inp,att)\n\n        main_loss = cse_loss(main_logits,targets)\n        sentiment_loss = mse_loss(sentiment_logits.view(-1),sentiment)\n        \n        if(pos_tagging==True and pos_scaff==True):\n            pos_loss = pos_cse_loss(pos_logits,pos_tags)\n            pos_tags = pos_tags.cpu().detach().numpy()\n            _,pos_predictions = torch.max(pos_logits,dim=1)\n            pos_tr,pos_pr = remove_pad_pred_t1(pos_tags,pos_predictions)\n            pos_true += pos_tr\n            pos_pred += pos_pr\n            ep_pos_loss += pos_loss.item()\n            \n        _,main_predictions = torch.max(main_logits,dim=1)\n\n        targets = targets.cpu().detach().numpy()\n        main_predictions = main_predictions.cpu().detach().numpy()\n        sentiment = list(sentiment.cpu().detach().numpy())\n        sentiment_logits = list(sentiment_logits.view(-1).cpu().detach().numpy())\n        \n        sent_true += sentiment\n        sent_pred += sentiment_logits\n        \n        tr,pr = remove_pad_pred_t1(targets,main_predictions)\n        \n        true += tr\n        pred += pr\n\n        if(pos_scaff==False):\n            total_loss = lambd1*main_loss + lambd2*sentiment_loss\n        else:\n            total_loss = lambd1*main_loss + lambd2*sentiment_loss + lambd3*pos_loss\n            \n        total_loss.backward()\n        running_loss += total_loss.item()\n        ep_main_loss += main_loss.item()\n        ep_sentiment_loss += sentiment_loss.item()\n        \n        optimizer.step()\n        optimizer.zero_grad()\n        \n            \n    running_loss = running_loss/(i+1)\n    ep_main_loss = ep_main_loss/(i+1)\n    ep_sentiment_loss = ep_sentiment_loss/(i+1)\n\n    f1 = f1_score(true,pred,average='macro')\n    acc = accuracy_score(true, pred)\n    mae = mean_absolute_error(sent_true,sent_pred)\n    \n    if(pos_tagging==True and pos_scaff==True):\n        ep_pos_loss = ep_pos_loss/(i+1) \n#         pos_f1 = f1_score(pos_true,pos_pred,average='macro')\n#         pos_acc = accuracy_score(pos_true, pos_pred)\n        pos_f1 = None\n        pos_acc = None\n    \n    if(pos_scaff==False):\n        pos_f1 = 'DOES NOT EXIST'\n        pos_acc = 'DOES NOT EXIST'\n        ep_pos_loss = 'DOES NOT EXIST'\n        \n    print('\\nepoch : ',ep+1,' --','\\n','Combined loss : ',running_loss,'\\t','Main Task Loss : ',ep_main_loss,'\\t','Sentiment Task Loss : ',ep_sentiment_loss,'\\t','POS Task Loss : ',ep_pos_loss)\n    print('\\nMain Task : -- ','F1 : ',f1,'\\t','Acc : ',acc)\n    print('train confusion matrix :')\n    print(confusion_matrix(true,pred))\n    print('\\nSentiment Task : -- ','MAE : ',mae)\n    print('\\nPOS Task : -- ','F1 : ',pos_f1,'\\t','Acc : ',pos_acc)\n    \n    torch.save(model, f'/kaggle/working/{trans_model}_{data}_cue_only_MTL_sentiment_model_ep{ep+1}.pt')\n\n    # validation \n    main_val_loss,main_val_true,main_val_pred = evaluate(model,val_data_loader,task=1,pos_tagging=pos_tagging,pos_scaff=pos_scaff,sent_scaff=sent_scaff)\n        \n    main_val_f1 = f1_score(main_val_true,main_val_pred,average='macro')\n    main_val_acc = accuracy_score(main_val_true, main_val_pred)\n    print('\\nmain task val loss : ',main_val_loss,'\\t','main task val_f1 : ',main_val_f1,'\\t','main task val_acc : ',main_val_acc)\n    print('main task val confusion matrix :')\n    print(confusion_matrix(main_val_true,main_val_pred))\n    print('\\n')\n    \n    loss_list.append({'total_train_loss':running_loss,'main_train_loss':ep_main_loss,'main_val_loss':main_val_loss,'sentiment_train_loss':ep_sentiment_loss,'pos_train_loss':ep_pos_loss})","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ntotal_train_loss_list = [i['total_train_loss'] for i in loss_list]\nmain_train_loss_list = [i['main_train_loss'] for i in loss_list]\nsentiment_train_loss_list = [i['sentiment_train_loss'] for i in loss_list]\nmain_val_loss_list = [i['main_val_loss'] for i in loss_list]\nep_list = [i+1 for i in range(epochs)]\nplt.rcParams['figure.figsize'] = [10, 10]\n# plt.plot(ep_list,total_train_loss_list, label='total train loss')\nplt.plot(ep_list,main_train_loss_list, label='main task train loss')\n# plt.plot(ep_list,sentiment_train_loss_list, label='sentiment task train loss')\nplt.plot(ep_list,main_val_loss_list, label='main task val loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Run -- Results on HedgePeer Test Data\nprint('{trans_model} MODEL RESULTS ON {data} TEST DATA')\n# root = '/kaggle/working/'\nfor model_name in os.listdir(root):\n    model_path = root+model_name\n    if(model_name[-2:]!='pt' or model_name[:4]=='bert'):\n        continue\n    model = torch.load(model_path)\n    model.to(device)\n    test_loss,test_true,test_pred = evaluate(model,test_data_loader,task=1,pos_tagging=pos_tagging,pos_scaff=pos_scaff,sent_scaff=sent_scaff)\n\n    test_f1 = f1_score(test_true,test_pred,average='macro')\n    test_acc = accuracy_score(test_true, test_pred)\n    print(f'model : {model_name}')\n    print('test loss : ',test_loss,'\\t','test_f1 : ',test_f1,'\\t','test_acc : ',test_acc)\n    print('test confusion matrix :')\n    print(confusion_matrix(test_true,test_pred))\n    print('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Task 2: Span Detection**","metadata":{}},{"cell_type":"markdown","source":"**Step-1 : HedgePeer Task 2 data Prep**","metadata":{}},{"cell_type":"code","source":"## Run\ndatat2 = df.reset_index().drop(columns=['index']).rename(columns = {'Raw Sentence': 'sentence'})\ndatat2['Span'] = spans\ndatat2","metadata":{"execution":{"iopub.status.busy":"2022-04-20T11:24:37.777713Z","iopub.execute_input":"2022-04-20T11:24:37.778049Z","iopub.status.idle":"2022-04-20T11:24:38.214474Z","shell.execute_reply.started":"2022-04-20T11:24:37.778018Z","shell.execute_reply":"2022-04-20T11:24:38.213084Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"## Run\nsent2_hedpeer = datat2['Hedged Sentence'].to_list()\nspan2_hedpeer = datat2['Span'].to_list()\nsentiment = datat2['Sentiment Intensity'].to_list()\nsen_t2list = None\nsent2 = []\nspans = []\n\nfor i,d in enumerate(zip(sent2_hedpeer,span2_hedpeer)):\n    hs = d[0]\n    s = d[1]\n    if(hs.find('<h>')>-1):\n        hs = hs.replace('<span>','#')\n        hs = hs.replace('</span>','#')\n        hs = hs.replace('<h>','token[0]')\n        hs = hs.replace('</h>','')\n        s = s.replace('<h>','token[0]')\n        s = s.replace('</h>','')\n    elif(hs.find('<mh>')>-1):\n        hs = hs.replace('<span>','#')\n        hs = hs.replace('</span>','#')\n        hs = hs.replace('<mh>','token[1]')\n        hs = hs.replace('</mh>','')\n        s = s.replace('<mh>','token[1]')\n        s = s.replace('</mh>','')\n    if(type(s)!=str):\n        s = ''\n    sent2.append(hs)\n    spans.append(s)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T11:08:12.504864Z","iopub.execute_input":"2022-04-19T11:08:12.505223Z","iopub.status.idle":"2022-04-19T11:08:12.631295Z","shell.execute_reply.started":"2022-04-19T11:08:12.505192Z","shell.execute_reply":"2022-04-19T11:08:12.630424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Run\n## 0=out of scope, 1=in scope \n\nclass Biot2_dataset(Dataset):\n    def __init__(self,sentences,spans,trans_model,sentiment,tokenizer,max_len,pos_tagging,num_tags,pos_scaff):\n        self.sent = sentences\n        self.trans_model = trans_model\n        self.token = tokenizer\n        self.max = max_len\n        self.spans = spans\n        self.sentiment = sentiment\n        self.pos_tagging = pos_tagging\n        self.num_tags = num_tags\n        self.pos_scaff = pos_scaff\n    def __len__(self):\n        return len(self.sent)\n    def tokenids_gen(self):\n        targets = []\n        senids=[]\n        attention_masks=[]\n        pos_tags=[]\n        pad_token_ids = {'xlnet':5,'bert':0,'scibert':0}\n        for s,sc in zip(self.sent,self.spans):\n            encodings = tokenizer.encode_plus(s,\n                                  return_tensors='pt',\n                                  truncation=False,\n                                  return_token_type_ids=True,\n                                  return_attention_mask=True,\n                                  )\n            \n            att = list(encodings['attention_mask'][0])\n            senid = list(encodings['input_ids'][0])\n            att = [i.item() for i in att]\n            senid = [i.item() for i in senid]\n            k = [tokenizer.decode(i) for i in senid]\n\n            tar = [0 for i in range(len(k))]\n            if(sc is not ''):\n                idxstart = k.index('#')\n                idxend = k.index('#',idxstart+1)\n                tar[idxstart] = -1\n                tar[idxend] = -1\n                tar[idxstart+1:idxend] = [1 for i in range(idxend-idxstart-1)]\n            \n            \n                for i in range(2):\n                    idx = k.index('#')\n                    if(k[idx-1]==''):\n                        del k[idx-1:idx+1]\n                        del senid[idx-1:idx+1]\n                        del tar[idx-1:idx+1]\n                        del att[idx-1:idx+1]\n                    else:\n                        del k[idx]\n                        del senid[idx]\n                        del tar[idx]\n                        del att[idx]\n                \n            senid = [i for i,j in zip(senid,k) if re.search('[A-Za-z0-9]+', j)!=None]\n            tar = [i for i,j in zip(tar,k) if re.search('[A-Za-z0-9]+', j)!=None]\n            att = [i for i,j in zip(att,k) if re.search('[A-Za-z0-9]+', j)!=None]\n            k = [i for i in k if re.search('[A-Za-z0-9]+', i)!=None]\n            \n            if(len(k)!=len(tar)):\n                print(k)\n                print('#'*40)\n                \n            ## adding pad token at the end....\n            tar = tar+[0 for i in range(self.max - len(k))]\n            senid = senid+[pad_token_ids[self.trans_model] for i in range(self.max - len(k))]\n            att = att+[0 for i in range(self.max - len(k))]\n            \n            if(self.pos_tagging == True and self.pos_scaff == False):\n                tagged = nltk.pos_tag(k)\n                pos_tag_labels = [one_hot_labels[i[1]] for i in tagged]\n                pad_encod = [0 for i in range(self.num_tags)]\n                pos_tag_labels = pos_tag_labels+[pad_encod for i in range(self.max - len(k))]\n                pos_tags.append(pos_tag_labels)\n            \n            if(self.pos_tagging == True and self.pos_scaff == True):\n                tagged = nltk.pos_tag(k)\n                pos_tag_labels = [char_to_int[i[1]] for i in tagged]\n                pad_encod = self.num_tags\n                pos_tag_labels = pos_tag_labels+[pad_encod for i in range(self.max - len(k))]\n                pos_tags.append(pos_tag_labels)\n            \n            targets.append(tar)\n            senids.append(senid)\n            attention_masks.append(att)\n        return (senids,attention_masks,targets,self.sentiment,pos_tags)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T11:09:44.329432Z","iopub.execute_input":"2022-04-19T11:09:44.329746Z","iopub.status.idle":"2022-04-19T11:09:44.349892Z","shell.execute_reply.started":"2022-04-19T11:09:44.329717Z","shell.execute_reply":"2022-04-19T11:09:44.349057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Run : Choose tokenizer type first in inp_tokenizer\ntrans_model = 'xlnet'\ndata = 'hedgepeer'\npos_tagging=False\npos_scaff=False\nnum_tags=num_tags\n\ntokenizer1 = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n\ntokenizer2 = BertTokenizer.from_pretrained('bert-base-cased')\n\ntokenizer3 = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_cased')\n\ntokenizer_dict = {'xlnet':tokenizer1,'bert':tokenizer2,'scibert':tokenizer3}\n\ntokenizer = tokenizer_dict[trans_model]\n\n## Run\n# remove instances with length more than 100\nlen_more,sen_t2list,sent2,spans,sentiment = remove_big_instances(datat2,sen_t2list,sent2,spans,sentiment,tokenizer,100)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T11:10:35.773545Z","iopub.execute_input":"2022-04-19T11:10:35.773861Z","iopub.status.idle":"2022-04-19T11:10:58.585175Z","shell.execute_reply.started":"2022-04-19T11:10:35.773833Z","shell.execute_reply":"2022-04-19T11:10:58.584332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(len_more)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T11:12:12.041121Z","iopub.execute_input":"2022-04-19T11:12:12.041482Z","iopub.status.idle":"2022-04-19T11:12:12.047046Z","shell.execute_reply.started":"2022-04-19T11:12:12.04145Z","shell.execute_reply":"2022-04-19T11:12:12.045836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Run\n\nmax_len = 100\nbatch_size = 4\n\nif(data=='hedgepeer'):\n    # split (train,val,test) = (70%,20%,10%)\n    train_size = 40088\n    val_size = 11448\n    test_size = len(sent2)-val_size-train_size\n\nelse:\n    print('Wrong Dataset Entry!')\n    \ny12 = [(c,s) for c,s in zip(spans,sentiment)]\nsen_train, sen_test, y12_train, y12_test = train_test_split(sent2,y12,test_size=test_size, random_state=0)\nsen_train, sen_val, y12_train, y12_val = train_test_split(sen_train,y12_train,test_size=val_size, random_state=0)\n    \ntrain_data_loader = dataloader_gen(sen_train,y12_train,trans_model,tokenizer,max_len,batch_size,task=2,pos_tagging=pos_tagging,num_tags=num_tags,pos_scaff=pos_scaff)\nval_data_loader = dataloader_gen(sen_val,y12_val,trans_model,tokenizer,max_len,batch_size,task=2,pos_tagging=pos_tagging,num_tags=num_tags,pos_scaff=pos_scaff)\ntest_data_loader = dataloader_gen(sen_test,y12_test,trans_model,tokenizer,max_len,batch_size,task=2,pos_tagging=pos_tagging,num_tags=num_tags,pos_scaff=pos_scaff)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-04-19T11:13:38.327092Z","iopub.execute_input":"2022-04-19T11:13:38.327437Z","iopub.status.idle":"2022-04-19T11:14:35.0979Z","shell.execute_reply.started":"2022-04-19T11:13:38.327406Z","shell.execute_reply":"2022-04-19T11:14:35.097117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(sent2)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T11:11:57.639526Z","iopub.execute_input":"2022-04-19T11:11:57.639926Z","iopub.status.idle":"2022-04-19T11:11:57.646851Z","shell.execute_reply.started":"2022-04-19T11:11:57.63989Z","shell.execute_reply":"2022-04-19T11:11:57.646073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(sen_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T11:14:35.099622Z","iopub.execute_input":"2022-04-19T11:14:35.099937Z","iopub.status.idle":"2022-04-19T11:14:35.10553Z","shell.execute_reply.started":"2022-04-19T11:14:35.099904Z","shell.execute_reply":"2022-04-19T11:14:35.104439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Task 2 : Span detection Model**","metadata":{}},{"cell_type":"markdown","source":"**Simple Baselines**","metadata":{}},{"cell_type":"code","source":"## Run : XLNet model\nclass scoperes_model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.xlmodel = XLNetModel.from_pretrained('xlnet-base-cased')\n        self.lin = nn.Linear(768,2)\n        \n    def forward(self,x,att):\n        xl=self.xlmodel(x,attention_mask=att)[0]\n        xl = xl.view(-1,xl.shape[2])\n        lin = self.lin(xl)\n        return (lin)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T11:15:55.624345Z","iopub.execute_input":"2022-04-19T11:15:55.624659Z","iopub.status.idle":"2022-04-19T11:15:55.630157Z","shell.execute_reply.started":"2022-04-19T11:15:55.624629Z","shell.execute_reply":"2022-04-19T11:15:55.629243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Run : Bert model \nclass scoperes_model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = BertModel.from_pretrained('bert-base-cased')\n        self.lin = nn.Linear(768,2)\n        \n    def forward(self,x,att):\n        xl=self.model(x,attention_mask=att)[0]\n        xl = xl.view(-1,xl.shape[2])\n        lin = self.lin(xl)\n        return (lin)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Run : Scibert model \nclass scoperes_model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = AutoModel.from_pretrained('allenai/scibert_scivocab_cased')\n        self.lin = nn.Linear(768,2)\n        \n    def forward(self,x,att):\n        xl=self.model(x,attention_mask=att)[0]\n        xl = xl.view(-1,xl.shape[2])\n        lin = self.lin(xl)\n        return (lin)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**MTL Model**","metadata":{}},{"cell_type":"code","source":"## Attention class\nclass attention(nn.Module):\n    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n        super(attention, self).__init__(**kwargs)\n        \n        self.supports_masking = True\n\n        self.bias = bias\n        self.feature_dim = feature_dim\n        self.step_dim = step_dim\n        self.features_dim = 0\n        self.a = 0\n        self.th = 0\n        self.eij = 0\n        \n        weight = torch.zeros(feature_dim, 1)\n        nn.init.kaiming_uniform_(weight)\n        self.weight = nn.Parameter(weight)\n        if bias:\n            self.b = nn.Parameter(torch.zeros(step_dim))\n        \n    def forward(self, x, mask=None):\n        feature_dim = self.feature_dim \n        step_dim = self.step_dim\n\n        self.eij = torch.mm(\n            x.contiguous().view(-1, feature_dim), \n            self.weight\n        ).view(-1, step_dim)\n        \n        if self.bias:\n            self.eij = self.eij + self.b\n            \n        self.th = torch.tanh(self.eij)\n        a = torch.exp(self.th)\n        if mask is not None:\n            a = a * mask\n\n        self.a = a / (torch.sum(a, 1, keepdim=True) + 1e-10)\n        weighted_input = x * torch.unsqueeze(self.a, -1)\n        return torch.sum(weighted_input, 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## MAIN TASK WITHOUT POS_TAGGING........\n# feed forward of main task (hedge cue/span prediction)\n\nclass main_head_1(nn.Module):\n    def __init__(self,lstm_hidden_size):\n        super().__init__()\n        self.lin = nn.Linear(2*lstm_hidden_size,16)\n        self.drop = nn.Dropout(p=0.4)\n        self.out = nn.Linear(16,2)\n        self.relu = torch.nn.ReLU()\n    def forward(self,lstm):\n        lstm = torch.reshape(lstm, (-1, lstm.shape[2]))\n        drop = self.drop(lstm)\n        lin = self.lin(drop)\n        lin = self.relu(lin)\n        out = self.out(lin)\n        return (out) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## MAIN TASK WITH POS_TAGGING........\n# feed forward of main task (hedge cue/span prediction)\n\nclass main_head_2(nn.Module):\n    def __init__(self,lstm_hidden_size,pos_tagging,num_tags):\n        super().__init__()\n        self.lin = nn.Linear(2*lstm_hidden_size,16)\n        self.out = nn.Linear(16+num_tags,2)\n        self.pos_tagging = pos_tagging\n    def forward(self,pos_tags,lstm):\n        lstm = torch.reshape(lstm, (-1, lstm.shape[2]))\n        pos_tags = pos_tags.view(-1,pos_tags.shape[2])\n        lin = self.lin(lstm)\n        pos_lin = torch.cat((lin,pos_tags),1).float()\n        out = self.out(pos_lin)\n        return (out) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feed forward of sentiment task \nclass sentiment_head(nn.Module):\n    def __init__(self,hidden_size,max_len):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.att = attention(2*hidden_size,max_len)   ## here second argument is max_len (here max_len = 100)\n        self.out = nn.Linear(2*hidden_size,1)\n    def forward(self,lstm):\n        at = self.att(lstm).view(-1,2*self.hidden_size)\n        x = self.out(at)\n        return (x) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## POS TASK........\n# feed forward of pos task\nclass pos_head(nn.Module):\n    def __init__(self,lstm_hidden_size,num_tags):\n        super().__init__()\n        self.lin = nn.Linear(2*lstm_hidden_size,num_tags)\n    def forward(self,lstm):\n        lstm = torch.reshape(lstm, (-1, lstm.shape[2]))\n        lin = self.lin(lstm)\n        return (lin) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## MTL model\nclass scope_MTL_model(nn.Module):\n    def __init__(self,trans_model,hidden_size,max_len,num_tags,pos_tagging=False,pos_scaff=False):\n        super().__init__()\n        if(trans_model=='xlnet'):\n            self.trans = XLNetModel.from_pretrained('xlnet-base-cased')\n        if(trans_model=='scibert'):\n            self.trans = AutoModel.from_pretrained('allenai/scibert_scivocab_cased')\n        if(trans_model=='bert'):\n            self.trans = BertModel.from_pretrained('bert-base-cased')\n        \n        self.lstm = nn.LSTM(768,hidden_size,num_layers=1,bidirectional=True,batch_first=True)\n        \n        if(pos_tagging==True and pos_scaff==False):\n            self.main = main_head_2(lstm_hidden_size=hidden_size,pos_tagging=pos_tagging,num_tags=num_tags)\n        else:\n            self.main = main_head_1(lstm_hidden_size=hidden_size)\n        \n        if(pos_tagging==True and pos_scaff==True):\n            self.pos = pos_head(lstm_hidden_size=hidden_size,num_tags=num_tags)\n            \n        self.sentiment = sentiment_head(hidden_size,max_len)\n        self.pos_tagging = pos_tagging\n        \n    def forward(self,x,att,pos_tags=None):\n        xl=self.trans(x,attention_mask=att)[0]\n        lstm,_=self.lstm(xl)\n        if(self.pos_tagging==True and pos_scaff==False):\n            main_out = self.main(pos_tags,lstm)\n        else:\n            main_out = self.main(lstm)\n        \n        sentiment_out = self.sentiment(lstm)\n        \n        if(pos_tagging==True and pos_scaff==True):\n            pos_out = self.pos(lstm)\n            return (main_out,sentiment_out,pos_out)\n            \n        else:\n            return (main_out,sentiment_out)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Training**","metadata":{}},{"cell_type":"markdown","source":"**Simple Baseline**","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = scoperes_model()\nmodel.to(device)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Run\n\ndef evaluate(model,val_data,task,pos_tagging,pos_scaff,sent_scaff):\n    model.eval()\n    model.to(device)\n    main_loss = 0\n    true=[]\n    pred=[]\n    with torch.no_grad():\n        for i,d in enumerate(val_data):\n            inp = d['input'].to(device)\n            att = d['attention_mask'].to(device)\n            targets = d['targets'].view(-1).to(device)\n            \n            if(pos_tagging==True and pos_scaff==False):\n                pos_tags = d['pos_tags'].to(device)\n                logits = model(inp,att,pos_tags)[0]\n            elif(sent_scaff==False and pos_tagging==False and pos_scaff==False):\n                logits = model(inp,att)   ## For simple baseline\n            else:\n                logits = model(inp,att)[0]   ## For sentiment MTL\n\n            loss = cse_loss(logits,targets)\n            main_loss += loss.item()\n            \n            _,predictions = torch.max(logits,dim=1)\n            \n            targets = targets.cpu().detach().numpy()\n            predictions = predictions.cpu().detach().numpy()\n            \n            if(task==1):\n                tr,pr = remove_pad_pred_t1(targets,predictions)\n                true += tr\n                pred += pr\n            else:\n                true += list(targets)\n                pred += list(predictions)\n                \n        main_loss = main_loss/(i+1)\n    return (main_loss,true,pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_pad_pred_t1(true,pred):\n    idx = [i for i,d in enumerate(true) if d==3]\n    true = [i for i in true if i!=3]\n    pred = [d for i,d in enumerate(pred) if i not in idx]\n    return (true,pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Run = Training -- Simple Baseline\n\nepochs = 7\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\ncse_loss = torch.nn.CrossEntropyLoss()\n\nloss_list=[]\nfor ep in range(epochs):\n    total_loss=0\n    true=[]\n    pred=[]\n    model.train()\n    \n    for i,d in enumerate(train_data_loader):\n        if(i%300 == 299):\n            print('batch - ',i+1)\n        \n        inp = d['input'].to(device)\n        att = d['attention_mask'].to(device)\n        targets = d['targets'].view(-1).to(device)\n\n        logits = model(inp,att)\n\n        loss = cse_loss(logits,targets)\n            \n        _,predictions = torch.max(logits,dim=1)\n\n        targets = targets.cpu().detach().numpy()\n        predictions = predictions.cpu().detach().numpy()\n        \n        true += list(targets)\n        pred += list(predictions)\n        \n        total_loss += loss.item()\n            \n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        \n            \n    total_loss = total_loss/(i+1)\n\n    f1 = f1_score(true,pred,average='macro')\n    acc = accuracy_score(true, pred)\n    print('epoch : ',ep+1,' --','\\n','loss : ',total_loss,'\\t','f1 : ',f1,'\\t','acc : ',acc)\n    print('train confusion matrix :')\n    print(confusion_matrix(true,pred))\n    \n    # validation \n    val_loss,val_true,val_pred = evaluate(model=model,val_data=val_data_loader,task=2,pos_tagging=False,pos_scaff=False,sent_scaff=False)\n        \n    val_f1 = f1_score(val_true,val_pred,average='macro')\n    val_acc = accuracy_score(val_true, val_pred)\n    print('val loss : ',val_loss,'\\t','val_f1 : ',val_f1,'\\t','val_acc : ',val_acc)\n    print('val confusion matrix :')\n    print(confusion_matrix(val_true,val_pred))\n    \n    torch.save(model, f'/kaggle/working/{trans_model}_{data}_span_only_model_ep{ep+1}.pt')\n    \n    loss_list.append({'train_loss':total_loss,'val_loss':val_loss})","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Run -- Results on HedgePeer Test Data\nprint('SCIBERT MODEL RESULTS ON HEDGEPEER TEST DATA')\nroot = './'\nfor model_name in os.listdir(root):\n    model_path = root+model_name\n    if(model_name[-2:]!='pt' or model_name[:4]=='bert'):\n        continue\n    model = torch.load(model_path)\n    model.to(device)\n    test_loss,test_true,test_pred = evaluate(model=model,val_data=test_data_loader,task=2,pos_tagging=False,pos_scaff=False,sent_scaff=False)\n\n    test_f1 = f1_score(test_true,test_pred,average='macro')\n    test_acc = accuracy_score(test_true, test_pred)\n    print(f'model : {model_name}')\n    print('test loss : ',test_loss,'\\t','test_f1 : ',test_f1,'\\t','test_acc : ',test_acc)\n    print('test confusion matrix :')\n    print(confusion_matrix(test_true,test_pred))\n    print('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cse_loss = torch.nn.CrossEntropyLoss()\nloss,truei,predi = evaluate(model,test_data_loader,task=task,pos_tagging=False,pos_scaff=False,sent_scaff=False)\n        \n# val_f1 = f1_score(val_true,val_pred,average='macro')\n# val_acc = accuracy_score(val_true, val_pred)\nprint(confusion_matrix(truei,predi))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**MTL Model**","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos_tagging = True\nhidden_size=100\npos_scaff = True\nsent_scaff = True\n\nif(pos_tagging == False and pos_scaff == False):\n    num_tags = None\n\nif(pos_tagging == True and pos_scaff == True):\n    num_tags = num_tags+1           # To take into account the label for the padding token.......\n\nmodel = scope_MTL_model(trans_model=trans_model,hidden_size=hidden_size,max_len=max_len,num_tags=num_tags,pos_tagging=pos_tagging,pos_scaff=pos_scaff)\nmodel.to(device)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Run --- Training Cue Detection Model -- MTL MODEL........\nepochs = 7\n\n# lambd1 = Lambda for main task, lambd2 = Lambda for sentiment task, lambd3 = Lambda for POS task\nlambd1 = 1\nlambd2 = 0.1\nlambd3 = 0.05\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\nif(pos_tagging==True and pos_scaff==True):\n    pos_weights = torch.tensor([float(1) for i in range(num_tags-1)]+[float(0)]).to(device)     # weight = 0 for padding token\n    pos_cse_loss = torch.nn.CrossEntropyLoss(weight=pos_weights)\ncse_loss = torch.nn.CrossEntropyLoss()\nmse_loss = torch.nn.MSELoss()\n\nloss_list=[]\nfor ep in range(epochs):\n    running_loss=0\n    ep_main_loss=0\n    ep_sentiment_loss=0\n    ep_pos_loss=0\n    true=[]\n    pred=[]\n    sent_true=[]\n    sent_pred=[]\n    pos_true=[]\n    pos_pred=[]\n    model.train()\n    \n    for i,d in enumerate(train_data_loader):\n        if(i%300 == 299):\n            print('batch - ',i+1)\n\n        inp = d['input'].to(device)\n        att = d['attention_mask'].to(device)\n        targets = d['targets'].view(-1).to(device)           # targets shape : (batch_size,max_len)\n        sentiment = d['sentiment'].to(device)\n        \n        if(pos_tagging==True):\n            pos_tags = d['pos_tags'].to(device)              # pos_tags shape for pos_scaff==False : (batch_size,max_len,num_tags)\n            if(pos_scaff==False):\n                main_logits,sentiment_logits = model(inp,att,pos_tags)\n            else:\n                pos_tags = pos_tags.view(-1)                 # pos_tags shape before view : (batch_size,max_len)\n                main_logits,sentiment_logits,pos_logits = model(inp,att)\n\n        else:\n            main_logits,sentiment_logits = model(inp,att)\n\n        main_loss = cse_loss(main_logits,targets)\n        sentiment_loss = mse_loss(sentiment_logits.view(-1),sentiment)\n        \n        if(pos_tagging==True and pos_scaff==True):\n            pos_loss = pos_cse_loss(pos_logits,pos_tags)\n            pos_tags = pos_tags.cpu().detach().numpy()\n            _,pos_predictions = torch.max(pos_logits,dim=1)\n            pos_tr,pos_pr = remove_pad_pred_t1(pos_tags,pos_predictions)\n            pos_true += pos_tr\n            pos_pred += pos_pr\n            ep_pos_loss += pos_loss.item()\n            \n        _,main_predictions = torch.max(main_logits,dim=1)\n\n        targets = targets.cpu().detach().numpy()\n        main_predictions = main_predictions.cpu().detach().numpy()\n        sentiment = list(sentiment.cpu().detach().numpy())\n        sentiment_logits = list(sentiment_logits.view(-1).cpu().detach().numpy())\n        \n        sent_true += sentiment\n        sent_pred += sentiment_logits\n        \n        true += list(targets)\n        pred += list(main_predictions)\n\n        if(pos_scaff==False):\n            total_loss = lambd1*main_loss + lambd2*sentiment_loss\n        else:\n            total_loss = lambd1*main_loss + lambd2*sentiment_loss + lambd3*pos_loss\n            \n        total_loss.backward()\n        running_loss += total_loss.item()\n        ep_main_loss += main_loss.item()\n        ep_sentiment_loss += sentiment_loss.item()\n        \n        optimizer.step()\n        optimizer.zero_grad()\n        \n            \n    running_loss = running_loss/(i+1)\n    ep_main_loss = ep_main_loss/(i+1)\n    ep_sentiment_loss = ep_sentiment_loss/(i+1)\n\n    f1 = f1_score(true,pred,average='macro')\n    acc = accuracy_score(true, pred)\n    mae = mean_absolute_error(sent_true,sent_pred)\n    \n    if(pos_tagging==True and pos_scaff==True):\n        ep_pos_loss = ep_pos_loss/(i+1) \n#         pos_f1 = f1_score(pos_true,pos_pred,average='macro')\n#         pos_acc = accuracy_score(pos_true, pos_pred)\n        pos_f1 = None\n        pos_acc = None\n    \n    if(pos_scaff==False):\n        pos_f1 = 'DOES NOT EXIST'\n        pos_acc = 'DOES NOT EXIST'\n        ep_pos_loss = 'DOES NOT EXIST'\n        \n    print('\\nepoch : ',ep+1,' --','\\n','Combined loss : ',running_loss,'\\t','Main Task Loss : ',ep_main_loss,'\\t','Sentiment Task Loss : ',ep_sentiment_loss,'\\t','POS Task Loss : ',ep_pos_loss)\n    print('\\nMain Task : -- ','F1 : ',f1,'\\t','Acc : ',acc)\n    print('train confusion matrix :')\n    print(confusion_matrix(true,pred))\n    print('\\nSentiment Task : -- ','MAE : ',mae)\n    print('\\nPOS Task : -- ','F1 : ',pos_f1,'\\t','Acc : ',pos_acc)\n    \n    torch.save(model, f'/kaggle/working/{trans_model}_{data}_span_only_MTL_sentiment_POS_scaff_model_ep{ep+1}.pt')\n\n    # validation \n    main_val_loss,main_val_true,main_val_pred = evaluate(model,val_data_loader,task=2,pos_tagging=pos_tagging,pos_scaff=pos_scaff,sent_scaff=sent_scaff)\n        \n    main_val_f1 = f1_score(main_val_true,main_val_pred,average='macro')\n    main_val_acc = accuracy_score(main_val_true, main_val_pred)\n    print('\\nmain task val loss : ',main_val_loss,'\\t','main task val_f1 : ',main_val_f1,'\\t','main task val_acc : ',main_val_acc)\n    print('main task val confusion matrix :')\n    print(confusion_matrix(main_val_true,main_val_pred))\n    print('\\n')\n    \n    loss_list.append({'total_train_loss':running_loss,'main_train_loss':ep_main_loss,'main_val_loss':main_val_loss,'sentiment_train_loss':ep_sentiment_loss,'pos_train_loss':ep_pos_loss})","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ntotal_train_loss_list = [i['total_train_loss'] for i in loss_list]\nmain_train_loss_list = [i['main_train_loss'] for i in loss_list]\nsentiment_train_loss_list = [i['sentiment_train_loss'] for i in loss_list]\nmain_val_loss_list = [i['main_val_loss'] for i in loss_list]\nep_list = [i+1 for i in range(epochs)]\nplt.rcParams['figure.figsize'] = [10, 10]\n# plt.plot(ep_list,total_train_loss_list, label='total train loss')\nplt.plot(ep_list,main_train_loss_list, label='main task train loss')\n# plt.plot(ep_list,sentiment_train_loss_list, label='sentiment task train loss')\nplt.plot(ep_list,main_val_loss_list, label='main task val loss')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Run -- Results on HedgePeer Test Data\nprint(f'{trans_model} MODEL RESULTS ON {data} TEST DATA')\n# root = '../input/cue-only-models/'\nroot = '/kaggle/working/'\nfor model_name in os.listdir(root):\n    model_path = root+model_name\n    if(model_name[-2:]!='pt' or model_name[:4]!='bert'):\n        continue\n#     if(model_name[:17]!='scibert_hedgepeer'):\n#         continue\n    model = torch.load(model_path)\n    model.to(device)\n    test_loss,test_true,test_pred = evaluate(model,test_data_loader,task=1,pos_tagging=pos_tagging,pos_scaff=pos_scaff,sent_scaff=sent_scaff)\n\n    test_f1 = f1_score(test_true,test_pred,average='macro')\n    test_acc = accuracy_score(test_true, test_pred)\n    print(f'model : {model_name}')\n    print('test loss : ',test_loss,'\\t','test_f1 : ',test_f1,'\\t','test_acc : ',test_acc)\n    print('test confusion matrix :')\n    print(confusion_matrix(test_true,test_pred))\n    print('\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}